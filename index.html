<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation.">
  <meta name="keywords" content="SIG-Chat, gesture generation, conversational gesture, spatial intent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <!-- 预留导航位 -->
</nav>

<!-- ================= HERO（白底，单列） ================= -->
<section class="hero is-medium">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="has-text-centered">
        <h1 class="title is-1 publication-title">
          SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation
        </h1>
        <p class="subtitle is-4 mt-3">
          Conversational gestures involving <strong>How</strong>, <strong>When</strong> and <strong>Where</strong>.
        </p>

        <!-- Authors -->
        <div class="is-size-6 publication-authors mt-4">
          <span class="author-block">
            <a>Yiheng Huang</a><sup>1,2,5*</sup>,</span>
          <span class="author-block">
            <a>Junran Peng</a><sup>2,5*✉</sup>,</span>
          <span class="author-block">
            <a>Wei Sui</a><sup>3↑</sup>,
          </span>
          <span class="author-block">
            <a>Silei Shen</a><sup>2,5</sup>,
          </span>
          <span class="author-block">
            <a>Jingwei Yang</a><sup>7</sup>,
          </span>
          <span class="author-block">
            <a>Chenghua Zhong</a><sup>2,3</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Zeji Wei</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Chencheng Bai</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Yonghao He</a><sup>3</sup>,
          </span>
          <span class="author-block">
            <a>Muyi Sun</a><sup>1</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Yan Liu</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Xu-Cheng Yin</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Man Zhang</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a>Chuanchen Luo</a><sup>4,5</sup>,
          </span>
        </div>

        <div class="is-size-6 mt-2">
          <p>
            <sup>∗</sup>: Equal contribution.&nbsp;&nbsp;
            <sup>↑</sup>: Project leader.&nbsp;&nbsp;
            <sup>✉</sup>: Corresponding author.
          </p>
        </div>

        <div class="is-size-6 publication-authors mt-3">
          <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
          <span class="author-block"><sup>2</sup>University of Science and Technology Beijing,</span>
          <span class="author-block"><sup>3</sup>D-Robotics,</span>
          <span class="author-block"><sup>4</sup>Shandong University,</span>
          <span class="author-block"><sup>5</sup>Linketic,</span>
          <span class="author-block"><sup>6</sup>Institute of Automation, Chinese Academy of Sciences,</span>
          <span class="author-block"><sup>7</sup>China University of Mining And Technology</span>
        </div>

        <!-- Buttons -->
        <div class="column has-text-centered mt-4">
          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2509.23852"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2509.23852"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
                </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-images"></i>
                </span>
                <span>Data (Coming Soon)</span>
              </a>
            </span>
          </div>
        </div>

        <!-- Teaser image -->
        <div class="mt-5">
          <figure class="image">
            <img src="./static/images/teaser.png" alt="SIG-Chat teaser">
          </figure>
          <p class="is-size-7 has-text-grey mt-2">
            SIG-Chat generates spatially grounded co-speech gestures that align with speech, semantics, and 3D interaction targets.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ DIVERSE GESTURES（紧凑视频网格） ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Diverse Conversational Gestures</h2>
    <p class="subtitle is-6 has-text-centered mb-5">
      SIG-Chat produces natural pointing and gaze behaviors across different speakers and scenes.
    </p>

    <!-- 紧凑网格 -->
    <div class="columns is-multiline is-variable is-2">
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-072_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-064_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-131_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-147_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/bcc1215pm-068_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/mxt1207am-003_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ ABSTRACT + STRUCTURE ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="box">
          <h2 class="title is-3 has-text-centered">Abstract</h2>
          <div class="content has-text-justified is-size-6">
            <p>
              The accompanying actions and gestures in dialogue are often closely linked to
              interactions with the environment, such as looking toward the interlocutor or using
              gestures to point to the described target at appropriate moments. Speech and semantics
              guide the production of gestures by determining their timing (WHEN) and style (HOW),
              while the spatial locations of interactive objects dictate their directional execution (WHERE).
            </p>
            <p>
              Existing approaches either rely solely on descriptive language to generate motions or utilize
              audio to produce non-interactive gestures, thereby lacking the characterization of interactive
              timing and spatial intent. This significantly limits the applicability of conversational gesture
              generation, whether in robotics or in the fields of game and animation production.
            </p>
            <p>
              To address this gap, we present a full-stack solution. We first establish a unique data collection
              method to simultaneously capture high-precision human motion and spatial intent. We then develop
              a generation model driven by audio, language, and spatial data, alongside dedicated metrics for
              evaluating interaction timing and spatial accuracy. Finally, we deploy the solution on a humanoid
              robot, enabling rich, context-aware physical interactions. Our data, models, and deployment
              solutions will be fully released.
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- Structure Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-10">
        <h2 class="title is-3">Structure Overview</h2>
        <div class="content">
          <figure class="image">
            <img src="./static/images/struct.png" alt="Structure Overview">
          </figure>
          <p class="is-size-7 has-text-grey mt-2">
            Figure1:Architecture of the denoising network. The model is a multi-layer diffusion-transformer
            with two fusion modules (speech fusion and intent fusion) that integrates audio, transcript, initial
            posture description, intent category, and target trajectory as multimodal conditions to estimate the
            clean gesture x̂0 from noisy gesture xt at diffusion timestep t. Multimodal inputs are processed and
            encoded by specific encoders. FC refers to the Fully Connected (FC) layer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ============ KEY COPONENT ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered mb-5">Key Components</h2>

    <!-- Card 1: Dataset -->
    <div class="card key-card mb-5">
      <div class="card-content">
        <div class="columns is-vcentered">
          <!-- Text left -->
          <div class="column is-7">
            <h3 class="title is-4">3.1 Dataset: SIG-Chat Multi-Modal Corpus</h3>
            <p class="is-size-6 has-text-justified">
              SIG-Chat is the first multi-modal conversational gesture dataset with explicit spatial interaction intents.
              Each sample is annotated with intent type, target location, and interaction timestamps, enabling precise
              modeling of <em>where</em> and <em>when</em> gestures should occur.
            </p>
            <ul class="is-size-7 has-text-grey mt-2">
              <li>• 11 hours, 7,123 gesture sequences with gaze and left/right pointing behaviors.</li>
              <li>• Captured with Xsens MVN Link and HTC Tracker, synchronizing audio, motion, and 3D target trajectories.</li>
              <li>• Each sample includes audio, transcript, motion, initial pose text, intent category, 3D trajectory, and timestamps.</li>
              <li>• 15 types of static/dynamic targets, with decoupled spatial labels for cross-setup and robot deployment.</li>
            </ul>
          </div>
          <!-- Image right -->
          <div class="column is-5">
            <figure class="image is-4by3 key-card-image">
              <!-- TODO: replace with your dataset figure -->
              <img src="./static/images/dataset.png" alt="SIG-Chat dataset overview">
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Card 2: Baseline -->
    <div class="card key-card mb-5">
      <div class="card-content">
        <div class="columns is-vcentered">
          <!-- Text left -->
          <div class="column is-7">
            <h3 class="title is-4">3.2 Baseline: Intent-aware Gesture Synthesis</h3>
            <p class="is-size-6 has-text-justified">
              We propose an intent-aware baseline built on a Diffusion Transformer (DiT) to generate gestures that are
              semantically aligned, rhythm-synchronized, and spatially precise.
            </p>
            <ul class="is-size-7 has-text-grey mt-2">
              <li>• Inputs: audio rhythm (WavLM), transcript semantics (FastText), intent category, initial pose description (CLIP), 3D target trajectory.</li>
              <li>• Multi-modal encoders align all modalities with the gesture sequence at 20 FPS.</li>
              <li>• Speech-Aware and Intent-Aware attention inject temporal rhythm and spatial intent into gesture features.</li>
              <li>• The model learns gestures that match what is said, how it is said, and where it refers to in 3D space.</li>
            </ul>
          </div>
          <!-- Image right -->
          <div class="column is-5">
            <figure class="image is-4by3 key-card-image">
              <!-- TODO: replace with your model architecture figure -->
              <img src="./static/images/model.png" alt="Intent-aware gesture synthesis baseline">
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Card 3: Metrics -->
    <!-- Key Component 3: Metrics (text top, image bottom) -->
    <div class="card key-card">
      <div class="card-content">
        <h3 class="title is-4">3.3 Metrics: Evaluating Spatial-Intent Gestures</h3>
        <p class="is-size-6 has-text-justified">
          We design a metric suite to evaluate both spatial and temporal accuracy of interaction gestures, capturing
          whether intents are executed at the right place and at the right time.
        </p>
        <ul class="is-size-7 has-text-grey mt-2">
          <li>• <strong>Intent Angular Deviation (IAD)</strong>: angle between execution direction and target direction.</li>
          <li>• Gaze: head/face orientation vs. eye-to-target vector; pointing: arm/hand direction vs. wrist-to-target vector.</li>
          <li>• <strong>IAR@k</strong>: percentage of frames within an angular threshold (e.g., 15° for pointing, 30° for gaze).</li>
          <li>• <strong>IoU@k</strong>: temporal overlap between predicted interaction frames and ground-truth segments.</li>
          <li>• FGD, Beat Consistency, and Diversity further assess motion naturalness, rhythm alignment, and variability.</li>
        </ul>

        <figure class="image key-card-image mt-4">
          
          <img src="./static/images/metrics.png" alt="Gesture interaction evaluation metrics">
        </figure>
      </div>
    </div>

  </div>
</section>

<!-- ============ EXPERIMENT ============ -->
 <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered mb-5">Experimental Results</h2>

    <!-- Card 1 -->
    <div class="box mb-5" style="padding: 2.5rem;">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <h3 class="title is-4">Quantitative Performance</h3>
          <p class="is-size-6 has-text-justified">
            Our intent-aware baseline significantly surpasses the non-intent model on both gaze and pointing gestures. 
            For visual intent, IAR@30° improves from 0.473 to 0.710, approaching the ground truth 0.808. 
            For pointing intent, the minimum IAD decreases from 47.75° to 5.15°, nearly matching the ground truth 4.99°, while IAR@15° improves from 0.204 to 0.846 and IoU@15° from 0.164 to 0.637. 
            FGD also drops substantially, showing that the generated gestures are not only spatially precise but also natural and realistic.
          </p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/exp1.png" style="max-width: 90%; border-radius: 10px;">
        </div>
      </div>
    </div>

    <!-- Card 2 -->
    <div class="box mb-5" style="padding: 2.5rem;">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <h3 class="title is-4">User Study</h3>
          <p class="is-size-6 has-text-justified">
            A user study with 20 participants on 50 audio–gesture pairs shows a clear preference for our model. 
            Compared with the w/o intent version, 78.7% of participants prefer our method for perceptual accuracy, and 70.8% prefer it for speech–gesture fluency. 
            When compared with ground truth motions, our model achieves competitive preference rates, demonstrating that its perceptual quality is often close to human gestures.
          </p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/exp2.png" style="max-width: 90%; border-radius: 10px;">
        </div>
      </div>
    </div>

    <!-- Conclusion: centered standalone card -->
    <div class="box" style="padding: 2rem 2.5rem;">
      <h3 class="title is-4 has-text-centered">Conclusion</h3>
      <p class="is-size-7 has-text-centered" style="max-width: 640px; margin: 0.5rem auto 0;">
        SIG-Chat advances conversational gesture generation by jointly modeling gesture style (HOW), timing (WHEN), and spatial target (WHERE),
        enabling accurate, intentional, and context-aware embodied interactions across both virtual and robotic platforms.
      </p>
    </div>


  </div>
</section>

<!-- ablation_study -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Ablation Study</h2>

    <!-- C.1 Quantitative evaluation on BEATv2 -->
    <div class="columns is-vcentered mt-4">
      <div class="column is-7">
        <h3 class="title is-4">C.1 Generalization on BEATv2</h3>
        <p class="is-size-6 has-text-justified">
          To evaluate the generalizability of our framework beyond intent-driven scenarios, we compare the
          no-intent variant of our baseline with state-of-the-art co-speech gesture generation models on the
          BEATv2 dataset. In this setting, the model only sees standing, front-facing gestures without explicit
          interaction intent, and we focus on gesture naturalness, diversity, and audio co-occurrence quality.
        </p>
        <p class="is-size-6 has-text-justified mt-2">
          The no-intent baseline keeps the speech backbone and Transformer architecture but removes all
          intent-specific components. Trained and evaluated on English speaker 2 of BEATv2, it is assessed
          using FGD, Diversity, and Beat Consistency, following existing benchmarks. The quantitative
          results table shows that our model achieves competitive or superior performance: it matches or
          approaches strong baselines such as EMAGE in terms of naturalness (low FGD), while achieving
          higher Beat Consistency, indicating a better alignment between gesture motion and speech rhythm.
        </p>
      </div>
      <div class="column is-5 has-text-centered">
        <!-- TODO: replace with your BEATv2 ablation figure or table screenshot -->
        <figure class="image">
          <img src="./static/images/abl_beatv2.png" alt="Ablation on BEATv2">
        </figure>
      </div>
    </div>

    <!-- C.2 Mixture training schemes -->
    <div class="columns is-vcentered mt-5">
      <div class="column is-7">
        <h3 class="title is-4">C.2 Mixture Training Schemes</h3>
        <p class="is-size-6 has-text-justified">
          We analyze different training schemes to validate our mixture batch sampling strategy. The
          compared schemes include: (1) <em>Batch resampling</em> (ours), which forms each batch with a fixed
          80/20 ratio of Track-II to Track-I samples; (2) <em>Uniform sampling</em>, which simply mixes all data
          from both tracks into a single pool; and (3) <em>Two-stage training</em>, which first pre-trains on
          Track-I and then fine-tunes on Track-II.
        </p>
        <p class="is-size-6 has-text-justified mt-2">
          As summarized in the quantitative results table, batch resampling consistently achieves the best
          performance on most key metrics, especially on Track-II, where interaction timing and spatial
          precision are directly evaluated via IAR and IoU. Uniform sampling struggles on Track-II, confirming
          that the model fails to fully learn from the underrepresented interactive examples. The two-stage
          scheme also underperforms our method, likely due to partial catastrophic forgetting during fine-tuning.
          In contrast, mixture sampling exposes the model to both tracks within each step, leading to a unified
          representation that is simultaneously generalizable and specialized for interactive gestures.
        </p>
      </div>
      <div class="column is-5 has-text-centered">
        <!-- TODO: replace with your mixture training ablation figure/table -->
        <figure class="image">
          <img src="./static/images/abl_mixture.png" alt="Ablation on mixture training schemes">
        </figure>
      </div>
    </div>

    <!-- C.3 Multimodal fusion schemes -->
    <div class="columns is-vcentered mt-5">
      <div class="column is-7">
        <h3 class="title is-4">C.3 Multimodal Fusion Schemes</h3>
        <p class="is-size-6 has-text-justified">
          We further compare three multimodal fusion schemes for integrating conditional inputs into the
          Diffusion Transformer. Our method uses a <em>two-step fusion</em> design, with separate cross-attention
          modules for audio-related features (speech and transcripts) and intent-related features (targets and
          intent category) in each DiT block. The second scheme, <em>one-step fusion</em>, merges all conditioning
          features into a single sequence and uses a single cross-attention module. The third, <em>pre-fusion</em>,
          concatenates all conditional embeddings with the noisy gesture embeddings at the input, without
          dedicated cross-attention.
        </p>
        <p class="is-size-6 has-text-justified mt-2">
          The quantitative results table clearly shows that two-step fusion achieves the best overall performance,
          particularly on Track-II metrics such as IAR and IoU that reflect spatially and temporally precise
          interactions. One-step fusion remains competitive but slightly weaker, suggesting that mixing all
          modalities in a single attention stream limits the model’s ability to disentangle temporal and spatial
          cues. The pre-fusion baseline performs significantly worse, indicating that dynamic, layer-wise
          cross-attention is crucial for effective multimodal conditioning in this task.
        </p>
      </div>
      <div class="column is-5 has-text-centered">
        <!-- TODO: replace with your fusion ablation figure/table -->
        <figure class="image">
          <img src="./static/images/abl_fusion.png" alt="Ablation on multimodal fusion schemes">
        </figure>
      </div>
    </div>

    <!-- C.4 Spatial representation of intent target -->
    <div class="columns is-vcentered mt-5">
      <div class="column is-7">
        <h3 class="title is-4">C.4 Spatial Representation of Intent Targets</h3>
        <p class="is-size-6 has-text-justified">
          Finally, we ablate different spatial representations of the intent target, comparing several coordinate
          systems for encoding 3D locations. The evaluated options include raw global translation (XYZ),
          horizontal unit vectors with distance and height, polar coordinates on the XZ-plane with height,
          and full spherical coordinates with azimuth, elevation, and distance.
        </p>
        <p class="is-size-6 has-text-justified mt-2">
          The quantitative results table shows that no single representation dominates across both Track-I and
          Track-II. Some specialized encodings, such as spherical coordinates, bring small gains on certain
          metrics for explicit interaction data, but their performance on more general data is less stable.
          We therefore adopt global translation as the default representation: it is simple, expressive enough,
          and yields consistently strong results across tracks without introducing additional inductive biases.
          This makes it a robust and practical choice for spatially intent-guided gesture generation.
        </p>
      </div>
      <div class="column is-5 has-text-centered">
        <!-- TODO: replace with your spatial representation ablation figure/table -->
        <figure class="image">
          <img src="./static/images/abl_spatial.png" alt="Ablation on spatial representations of intent targets">
        </figure>
      </div>
    </div>

  </div>
</section>


<!-- ============ GENERATE DEMOS（4个一行，小视频 + 文本） ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Generate Demos</h2>

    <!-- Looking Gestures -->
    <div class="mt-4">
      <h3 class="title is-4">Looking Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat generates gaze behaviors that consistently look towards the referenced target.
      </p>
      <div class="columns is-multiline is-variable is-1">
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              I can hear the subway braking. <br>
              There, that train is gliding to the platform.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Oh... that's too bad. Bye bye. <br>
              (She walks away towards the door.)
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The chopper’s rotor is getting louder. <br>
              Look at that, the helicopter is coming along the river.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Excuse me. Doesn't this come with a side salad?
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <!-- Pointing Gestures -->
    <div class="mt-5">
      <h3 class="title is-4">Pointing Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat produces pointing gestures that align with the spatial locations of interaction targets.
      </p>
      <div class="columns is-multiline is-variable is-1">
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              That file you’re asking about? I think I saw it on the desk yesterday... <br>
              Wait a second, yeah, it’s over there.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Hey, guess who I just saw? <br>
              Right over there.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The novel you wanted is in the fiction section— <br>
              look, that shelf with the blue spines!
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The ball just rolled off—see? <br>
              It’s bouncing toward the left side!
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ HUMANOID ROBOT DEPLOYMENT ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Deploy on Humanoid Robot</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            We further deploy our motion controller on the Unitree G1 robot, integrating a YOLO-World model
            for attention-target detection and a reinforcement-learning policy that enables the robot to orient
            toward the detected target.
          </p>
        </div>

        <div class="columns is-centered is-variable is-2">
          <div class="column">
            <div class="content has-text-centered">
              <div class="video-thumb">
                <video autoplay controls muted loop playsinline>
                  <source src="./static/videos/find_phone.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <div class="column">
            <div class="content has-text-centered">
              <div class="video-thumb">
                <video controls playsinline muted>
                  <source src="./static/videos/drunk.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ BIBTEX ============ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{huang2025sigchatspatialintentguidedconversational,
  title={SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where},
  author={Yiheng Huang and Junran Peng and Silei Shen and Jingwei Yang and Zeji Wei and Chencheng Bai and Yonghao He and Wei Sui and Muyi Sun and Yan Liu and Xu-Cheng Yin and Man Zhang and Zhaoxiang Zhang and Chuanchen Luo},
  year={2025},
  eprint={2509.23852},
  archivePrefix={arXiv},
  primaryClass={cs.GR},
  url={https://arxiv.org/abs/2509.23852},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/2509.23852v3.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project page</a>.
            You are free to borrow the source code; please remember to remove the analytics code if you do not need it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
