<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation.">
  <meta name="keywords" content="SIG-Chat, gesture generation, conversational gesture, spatial intent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <!-- 预留导航位 -->
</nav>

<!-- ================= HERO（白底，单列） ================= -->
<section class="hero is-medium">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="has-text-centered">
        <h1 class="title is-1 publication-title">
          SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation
        </h1>
        <p class="subtitle is-4 mt-3">
          Conversational gestures involving <strong>How</strong>, <strong>When</strong> and <strong>Where</strong>.
        </p>

        <!-- Authors -->
        <div class="is-size-6 publication-authors mt-4">
          <span class="author-block">
            <a>Yiheng Huang</a><sup>1,2,5*</sup>,</span>
          <span class="author-block">
            <a>Junran Peng</a><sup>2,5*✉</sup>,</span>
          <span class="author-block">
            <a>Wei Sui</a><sup>3↑</sup>,
          </span>
          <span class="author-block">
            <a>Silei Shen</a><sup>2,5</sup>,
          </span>
          <span class="author-block">
            <a>Jingwei Yang</a><sup>7</sup>,
          </span>
          <span class="author-block">
            <a>Chenghua Zhong</a><sup>2,3</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Zeji Wei</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Chencheng Bai</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Yonghao He</a><sup>3</sup>,
          </span>
          <span class="author-block">
            <a>Muyi Sun</a><sup>1</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Yan Liu</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Xu-Cheng Yin</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Man Zhang</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a>Chuanchen Luo</a><sup>4,5</sup>,
          </span>
        </div>

        <div class="is-size-6 mt-2">
          <p>
            <sup>∗</sup>: Equal contribution.&nbsp;&nbsp;
            <sup>↑</sup>: Project leader.&nbsp;&nbsp;
            <sup>✉</sup>: Corresponding author.
          </p>
        </div>

        <div class="is-size-6 publication-authors mt-3">
          <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
          <span class="author-block"><sup>2</sup>University of Science and Technology Beijing,</span>
          <span class="author-block"><sup>3</sup>D-Robotics,</span>
          <span class="author-block"><sup>4</sup>Shandong University,</span>
          <span class="author-block"><sup>5</sup>Linketic,</span>
          <span class="author-block"><sup>6</sup>Institute of Automation, Chinese Academy of Sciences,</span>
          <span class="author-block"><sup>7</sup>China University of Mining And Technology</span>
        </div>

        <!-- Buttons -->
        <div class="column has-text-centered mt-4">
          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2509.23852"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2509.23852"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
                </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-images"></i>
                </span>
                <span>Data (Coming Soon)</span>
              </a>
            </span>
          </div>
        </div>

        <!-- Teaser image -->
        <div class="mt-5">
          <figure class="image">
            <img src="./static/images/teaser.png" alt="SIG-Chat teaser">
          </figure>
          <p class="is-size-7 has-text-grey mt-2">
            SIG-Chat generates spatially grounded co-speech gestures that align with speech, semantics, and 3D interaction targets.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ DIVERSE GESTURES（紧凑视频网格） ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Diverse Conversational Gestures</h2>
    <p class="subtitle is-6 has-text-centered mb-5">
      SIG-Chat produces natural pointing and gaze behaviors across different speakers and scenes.
    </p>

    <!-- 紧凑网格 -->
    <div class="columns is-multiline is-variable is-2">
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-072_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-064_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-131_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-147_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/bcc1215pm-068_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <div class="video-thumb">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/mxt1207am-003_front.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ ABSTRACT + STRUCTURE ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="box">
          <h2 class="title is-3 has-text-centered">Abstract</h2>
          <div class="content has-text-justified is-size-6">
            <p>
              The accompanying actions and gestures in dialogue are often closely linked to
              interactions with the environment, such as looking toward the interlocutor or using
              gestures to point to the described target at appropriate moments. Speech and semantics
              guide the production of gestures by determining their timing (WHEN) and style (HOW),
              while the spatial locations of interactive objects dictate their directional execution (WHERE).
            </p>
            <p>
              Existing approaches either rely solely on descriptive language to generate motions or utilize
              audio to produce non-interactive gestures, thereby lacking the characterization of interactive
              timing and spatial intent. This significantly limits the applicability of conversational gesture
              generation, whether in robotics or in the fields of game and animation production.
            </p>
            <p>
              To address this gap, we present a full-stack solution. We first establish a unique data collection
              method to simultaneously capture high-precision human motion and spatial intent. We then develop
              a generation model driven by audio, language, and spatial data, alongside dedicated metrics for
              evaluating interaction timing and spatial accuracy. Finally, we deploy the solution on a humanoid
              robot, enabling rich, context-aware physical interactions. Our data, models, and deployment
              solutions will be fully released.
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- Structure Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-10">
        <h2 class="title is-3">Structure Overview</h2>
        <div class="content">
          <figure class="image">
            <img src="./static/images/struct.png" alt="Structure Overview">
          </figure>
          <p class="is-size-7 has-text-grey mt-2">
            Figure1:Architecture of the denoising network. The model is a multi-layer diffusion-transformer
            with two fusion modules (speech fusion and intent fusion) that integrates audio, transcript, initial
            posture description, intent category, and target trajectory as multimodal conditions to estimate the
            clean gesture x̂0 from noisy gesture xt at diffusion timestep t. Multimodal inputs are processed and
            encoded by specific encoders. FC refers to the Fully Connected (FC) layer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ============ KEY COPONENT ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered mb-5">Key Components</h2>

    <!-- Card 1: Dataset -->
    <div class="card key-card mb-5">
      <div class="card-content">
        <div class="columns is-vcentered">
          <!-- Text left -->
          <div class="column is-7">
            <h3 class="title is-4">3.1 Dataset: SIG-Chat Multi-Modal Corpus</h3>
            <p class="is-size-6 has-text-justified">
              SIG-Chat is the first multi-modal conversational gesture dataset with explicit spatial interaction intents.
              Each sample is annotated with intent type, target location, and interaction timestamps, enabling precise
              modeling of <em>where</em> and <em>when</em> gestures should occur.
            </p>
            <ul class="is-size-7 has-text-grey mt-2">
              <li>• 11 hours, 7,123 gesture sequences with gaze and left/right pointing behaviors.</li>
              <li>• Captured with Xsens MVN Link and HTC Tracker, synchronizing audio, motion, and 3D target trajectories.</li>
              <li>• Each sample includes audio, transcript, motion, initial pose text, intent category, 3D trajectory, and timestamps.</li>
              <li>• 15 types of static/dynamic targets, with decoupled spatial labels for cross-setup and robot deployment.</li>
            </ul>
          </div>
          <!-- Image right -->
          <div class="column is-5">
            <figure class="image is-4by3 key-card-image">
              <!-- TODO: replace with your dataset figure -->
              <img src="./static/images/dataset.png" alt="SIG-Chat dataset overview">
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Card 2: Baseline -->
    <div class="card key-card mb-5">
      <div class="card-content">
        <div class="columns is-vcentered">
          <!-- Text left -->
          <div class="column is-7">
            <h3 class="title is-4">3.2 Baseline: Intent-aware Gesture Synthesis</h3>
            <p class="is-size-6 has-text-justified">
              We propose an intent-aware baseline built on a Diffusion Transformer (DiT) to generate gestures that are
              semantically aligned, rhythm-synchronized, and spatially precise.
            </p>
            <ul class="is-size-7 has-text-grey mt-2">
              <li>• Inputs: audio rhythm (WavLM), transcript semantics (FastText), intent category, initial pose description (CLIP), 3D target trajectory.</li>
              <li>• Multi-modal encoders align all modalities with the gesture sequence at 20 FPS.</li>
              <li>• Speech-Aware and Intent-Aware attention inject temporal rhythm and spatial intent into gesture features.</li>
              <li>• The model learns gestures that match what is said, how it is said, and where it refers to in 3D space.</li>
            </ul>
          </div>
          <!-- Image right -->
          <div class="column is-5">
            <figure class="image is-4by3 key-card-image">
              <!-- TODO: replace with your model architecture figure -->
              <img src="./static/images/model.png" alt="Intent-aware gesture synthesis baseline">
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Card 3: Metrics -->
    <div class="card key-card">
      <div class="card-content">
        <div class="columns is-vcentered">
          <!-- Text left -->
          <div class="column is-7">
            <h3 class="title is-4">3.3 Metrics: Evaluating Spatial-Intent Gestures</h3>
            <p class="is-size-6 has-text-justified">
              We design a metric suite to evaluate both spatial and temporal accuracy of interaction gestures, capturing
              whether intents are executed at the right place and at the right time.
            </p>
            <ul class="is-size-7 has-text-grey mt-2">
              <li>• <strong>Intent Angular Deviation (IAD)</strong>: angle between execution direction and target direction.</li>
              <li>• Gaze: head/face orientation vs. eye-to-target vector; pointing: arm/hand direction vs. wrist-to-target vector.</li>
              <li>• <strong>IAR@k</strong>: percentage of frames within an angular threshold (e.g., 15° for pointing, 30° for gaze).</li>
              <li>• <strong>IoU@k</strong>: temporal overlap between predicted interaction frames and ground-truth segments.</li>
              <li>• FGD, Beat Consistency, and Diversity further assess motion naturalness, rhythm alignment, and variability.</li>
            </ul>
          </div>
          <!-- Image right -->
          <div class="column is-5">
            <figure class="image is-4by3 key-card-image">
              <!-- TODO: replace with your metrics / evaluation figure -->
              <img src="./static/images/metrics.png" alt="Gesture interaction evaluation metrics">
            </figure>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<!-- ============ EXPERIMENT ============ -->
 <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered mb-5">Experimental Results</h2>

    <!-- Card 1 -->
    <div class="box mb-5" style="padding: 2.5rem;">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <h3 class="title is-4">Quantitative Performance</h3>
          <p class="is-size-6 has-text-justified">
            Our intent-aware baseline significantly surpasses the non-intent model on both gaze and pointing gestures. 
            For visual intent, IAR@30° improves from 0.473 to 0.710, approaching the ground truth 0.808. 
            For pointing intent, the minimum IAD decreases from 47.75° to 5.15°, nearly matching the ground truth 4.99°, while IAR@15° improves from 0.204 to 0.846 and IoU@15° from 0.164 to 0.637. 
            FGD also drops substantially, showing that the generated gestures are not only spatially precise but also natural and realistic.
          </p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/exp1.png" style="max-width: 90%; border-radius: 10px;">
        </div>
      </div>
    </div>

    <!-- Card 2 -->
    <div class="box mb-5" style="padding: 2.5rem;">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <h3 class="title is-4">User Study</h3>
          <p class="is-size-6 has-text-justified">
            A user study with 20 participants on 50 audio–gesture pairs shows a clear preference for our model. 
            Compared with the w/o intent version, 78.7% of participants prefer our method for perceptual accuracy, and 70.8% prefer it for speech–gesture fluency. 
            When compared with ground truth motions, our model achieves competitive preference rates, demonstrating that its perceptual quality is often close to human gestures.
          </p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/exp2.png" style="max-width: 90%; border-radius: 10px;">
        </div>
      </div>
    </div>

    <!-- Card 3: Conclusion (one-line style) -->
    <div class="box" style="padding: 2.5rem;">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <h3 class="title is-4">Conclusion</h3>
          <p class="is-size-7 has-text-justified" style="max-width: 95%;">
            SIG-Chat advances conversational gesture generation by jointly modeling gesture style (HOW), timing (WHEN), and spatial target (WHERE), enabling accurate, intentional, and context-aware embodied interactions across both virtual and robotic platforms.
          </p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/exp3.png" style="max-width: 90%; border-radius: 10px;">
        </div>
      </div>
    </div>

  </div>
</section>




<!-- ============ GENERATE DEMOS（4个一行，小视频 + 文本） ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Generate Demos</h2>

    <!-- Looking Gestures -->
    <div class="mt-4">
      <h3 class="title is-4">Looking Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat generates gaze behaviors that consistently look towards the referenced target.
      </p>
      <div class="columns is-multiline is-variable is-1">
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              I can hear the subway braking. <br>
              There, that train is gliding to the platform.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Oh... that's too bad. Bye bye. <br>
              (She walks away towards the door.)
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The chopper’s rotor is getting louder. <br>
              Look at that, the helicopter is coming along the river.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Excuse me. Doesn't this come with a side salad?
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <!-- Pointing Gestures -->
    <div class="mt-5">
      <h3 class="title is-4">Pointing Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat produces pointing gestures that align with the spatial locations of interaction targets.
      </p>
      <div class="columns is-multiline is-variable is-1">
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              That file you’re asking about? I think I saw it on the desk yesterday... <br>
              Wait a second, yeah, it’s over there.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              Hey, guess who I just saw? <br>
              Right over there.
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The novel you wanted is in the fiction section— <br>
              look, that shelf with the blue spines!
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-one-quarter-desktop is-half-tablet">
          <div class="video-thumb small-thumb">
            <p class="is-size-7 has-text-grey mb-1">
              The ball just rolled off—see? <br>
              It’s bouncing toward the left side!
            </p>
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ HUMANOID ROBOT DEPLOYMENT ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Deploy on Humanoid Robot</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            We further deploy our motion controller on the Unitree G1 robot, integrating a YOLO-World model
            for attention-target detection and a reinforcement-learning policy that enables the robot to orient
            toward the detected target.
          </p>
        </div>

        <div class="columns is-centered is-variable is-2">
          <div class="column">
            <div class="content has-text-centered">
              <div class="video-thumb">
                <video autoplay controls muted loop playsinline>
                  <source src="./static/videos/find_phone.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <div class="column">
            <div class="content has-text-centered">
              <div class="video-thumb">
                <video controls playsinline muted>
                  <source src="./static/videos/drunk.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ BIBTEX ============ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{huang2025sigchatspatialintentguidedconversational,
  title={SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where},
  author={Yiheng Huang and Junran Peng and Silei Shen and Jingwei Yang and Zeji Wei and Chencheng Bai and Yonghao He and Wei Sui and Muyi Sun and Yan Liu and Xu-Cheng Yin and Man Zhang and Zhaoxiang Zhang and Chuanchen Luo},
  year={2025},
  eprint={2509.23852},
  archivePrefix={arXiv},
  primaryClass={cs.GR},
  url={https://arxiv.org/abs/2509.23852},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/2509.23852v3.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project page</a>.
            You are free to borrow the source code; please remember to remove the analytics code if you do not need it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
