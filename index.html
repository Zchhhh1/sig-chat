<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation.">
  <meta name="keywords" content="SIG-Chat, gesture generation, conversational gesture, spatial intent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <!-- 这里留空，如果以后想加导航可以在这儿加 -->
</nav>

<!-- ================= HERO ================= -->
<section class="hero is-primary is-medium">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="has-text-centered">
        <h1 class="title is-1 publication-title">
          SIG-Chat：Spatial Intent-Guided Conversational Gesture Generation
        </h1>
        <p class="subtitle is-4 mt-3">
          Conversational gestures involving <strong>How</strong>, <strong>When</strong> and <strong>Where</strong>.
        </p>

        <!-- Authors -->
        <div class="is-size-6 publication-authors mt-4">
          <span class="author-block">
            <a>Yiheng Huang</a><sup>1,2,5*</sup>,</span>
          <span class="author-block">
            <a>Junran Peng</a><sup>2,5*✉</sup>,</span>
          <span class="author-block">
            <a>Wei Sui</a><sup>3↑</sup>,
          </span>
          <span class="author-block">
            <a>Silei Shen</a><sup>2,5</sup>,
          </span>
          <span class="author-block">
            <a>Jingwei Yang</a><sup>7</sup>,
          </span>
          <span class="author-block">
            <a>Chenghua Zhong</a><sup>2,3</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Zeji Wei</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Chencheng Bai</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Yonghao He</a><sup>3</sup>,
          </span>
          <span class="author-block">
            <a>Muyi Sun</a><sup>1</sup>,
          </span>
        </div>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <a>Yan Liu</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Xu-Cheng Yin</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a>Man Zhang</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a>Chuanchen Luo</a><sup>4,5</sup>,
          </span>
        </div>

        <div class="is-size-6 mt-2">
          <p>
            <sup>∗</sup>: Equal contribution.&nbsp;&nbsp;
            <sup>↑</sup>: Project leader.&nbsp;&nbsp;
            <sup>✉</sup>: Corresponding author.
          </p>
        </div>

        <div class="is-size-6 publication-authors mt-3">
          <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
          <span class="author-block"><sup>2</sup>University of Science and Technology Beijing,</span>
          <span class="author-block"><sup>3</sup>D-Robotics,</span>
          <span class="author-block"><sup>4</sup>Shandong University,</span>
          <span class="author-block"><sup>5</sup>Linketic,</span>
          <span class="author-block"><sup>6</sup>Institute of Automation, Chinese Academy of Sciences,</span>
          <span class="author-block"><sup>7</sup>China University of Mining And Technology</span>
        </div>

        <!-- Buttons -->
        <div class="column has-text-centered mt-4">
          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2509.23852"
                 class="external-link button is-normal is-rounded is-light">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2509.23852"
                 class="external-link button is-normal is-rounded is-light">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-light is-outlined">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
                </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-light is-outlined">
                <span class="icon">
                    <i class="far fa-images"></i>
                </span>
                <span>Data (Coming Soon)</span>
              </a>
            </span>
          </div>
        </div>

        <!-- Teaser image (单列往下排，类似 MarketGen) -->
        <div class="mt-5">
          <figure class="image">
            <img src="./static/images/teaser.png" alt="SIG-Chat teaser">
          </figure>
          <p class="is-size-6 has-text-grey mt-2">
            SIG-Chat generates spatially grounded co-speech gestures that align with speech, semantics, and 3D interaction targets.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ DIVERSE GESTURES (紧凑视频网格) ============ -->
<section class="section has-background-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Diverse Conversational Gestures</h2>
    <p class="subtitle is-6 has-text-centered mb-5">
      SIG-Chat produces natural pointing and gaze behaviors across different speakers and scenes.
    </p>

    <div class="columns is-multiline">
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-072_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-064_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-131_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/0827am-147_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/bcc1215pm-068_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
      <div class="column is-one-third-desktop is-half-tablet">
        <figure class="image is-16by9">
          <video autoplay muted loop playsinline controls>
            <source src="./static/videos/mxt1207am-003_front.mp4" type="video/mp4">
          </video>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- ============ ABSTRACT + STRUCTURE ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="box">
          <h2 class="title is-3 has-text-centered">Abstract</h2>
          <div class="content has-text-justified is-size-6">
            <p>
              The accompanying actions and gestures in dialogue are often closely linked to
              interactions with the environment, such as looking toward the interlocutor or using
              gestures to point to the described target at appropriate moments. Speech and semantics
              guide the production of gestures by determining their timing (WHEN) and style (HOW),
              while the spatial locations of interactive objects dictate their directional execution (WHERE).
            </p>
            <p>
              Existing approaches either rely solely on descriptive language to generate motions or utilize
              audio to produce non-interactive gestures, thereby lacking the characterization of interactive
              timing and spatial intent. This significantly limits the applicability of conversational gesture
              generation, whether in robotics or in the fields of game and animation production.
            </p>
            <p>
              To address this gap, we present a full-stack solution. We first establish a unique data collection
              method to simultaneously capture high-precision human motion and spatial intent. We then develop
              a generation model driven by audio, language, and spatial data, alongside dedicated metrics for
              evaluating interaction timing and spatial accuracy. Finally, we deploy the solution on a humanoid
              robot, enabling rich, context-aware physical interactions. Our data, models, and deployment
              solutions will be fully released.
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- Structure Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-10">
        <h2 class="title is-3">Structure Overview</h2>
        <div class="content">
          <figure class="image">
            <img src="./static/images/struct.png" alt="Structure Overview">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ GENERATE DEMOS ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Generate Demos</h2>

    <!-- Looking Gestures -->
    <div class="box mt-4">
      <h3 class="title is-4">Looking Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat generates gaze behaviors that consistently look towards the referenced target.
      </p>
      <div class="columns is-multiline">
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze1.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze2.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze3.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/gaze4.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
      </div>
    </div>

    <!-- Pointing Gestures -->
    <div class="box mt-4">
      <h3 class="title is-4">Pointing Gestures</h3>
      <p class="is-size-6 has-text-grey mb-4">
        SIG-Chat produces pointing gestures that align with the spatial locations of interaction targets.
      </p>
      <div class="columns is-multiline">
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing1.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing2.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing3.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image is-16by9">
            <video autoplay muted loop playsinline controls>
              <source src="./static/videos/pointing4.mp4" type="video/mp4">
            </video>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============ HUMANOID ROBOT DEPLOYMENT ============ -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Deploy on Humanoid Robot</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            We further deploy our motion controller on the Unitree G1 robot, integrating a YOLO-World model
            for attention-target detection and a reinforcement-learning policy that enables the robot to orient
            toward the detected target.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column">
            <div class="content has-text-centered">
              <figure class="image is-16by9">
                <video autoplay controls muted loop playsinline>
                  <source src="./static/videos/find_phone.mp4" type="video/mp4">
                </video>
              </figure>
            </div>
          </div>
          <div class="column">
            <div class="content has-text-centered">
              <figure class="image is-16by9">
                <video controls playsinline muted>
                  <source src="./static/videos/drunk.mp4" type="video/mp4">
                </video>
              </figure>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- ============ BIBTEX ============ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{huang2025sigchatspatialintentguidedconversational,
  title={SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where},
  author={Yiheng Huang and Junran Peng and Silei Shen and Jingwei Yang and Zeji Wei and Chencheng Bai and Yonghao He and Wei Sui and Muyi Sun and Yan Liu and Xu-Cheng Yin and Man Zhang and Zhaoxiang Zhang and Chuanchen Luo},
  year={2025},
  eprint={2509.23852},
  archivePrefix={arXiv},
  primaryClass={cs.GR},
  url={https://arxiv.org/abs/2509.23852},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/2509.23852v3.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project page</a>.
            You are free to borrow the source code; please remember to remove the analytics code if you do not need it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
